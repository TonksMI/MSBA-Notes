# Class 1: Math Review + Introduction
**Date:** August 27, 2025 (Wednesday)  
**Quiz:** Yes (covering basic math concepts)

## Overview
Comprehensive review of essential mathematical concepts: linear algebra, probability theory, and statistics fundamentals needed for statistical machine learning.

## Linear Algebra Review

### Vector Spaces and Operations

#### Vector Basics
A vector $\mathbf{v}$ in $\mathbb{R}^n$ represents a point or direction in n-dimensional space:
$$\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$$

**Key Operations:**
- **Addition**: $\mathbf{u} + \mathbf{v} = \begin{bmatrix} u_1+v_1 \\ u_2+v_2 \\ \vdots \\ u_n+v_n \end{bmatrix}$
- **Scalar multiplication**: $c\mathbf{v} = \begin{bmatrix} cv_1 \\ cv_2 \\ \vdots \\ cv_n \end{bmatrix}$
- **Dot product**: $\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{n} u_i v_i$

**Example in ML Context:**
```
Feature vector: x = [age, income, education_years]·µÄ = [25, 50000, 16]·µÄ
Weight vector:  w = [0.1, 0.0001, 0.5]·µÄ
Prediction:     ≈∑ = w¬∑x = 0.1(25) + 0.0001(50000) + 0.5(16) = 15.5
```

#### Vector Norms
**L2 Norm (Euclidean length):**
$$\|\mathbf{v}\|_2 = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} = \sqrt{\mathbf{v} \cdot \mathbf{v}}$$

**L1 Norm (Manhattan distance):**
$$\|\mathbf{v}\|_1 = |v_1| + |v_2| + \cdots + |v_n|$$

**ML Application:** Regularization
- **Ridge regression**: Minimize $\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda\|\boldsymbol{\beta}\|_2^2$
- **Lasso regression**: Minimize $\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda\|\boldsymbol{\beta}\|_1$

### Matrix Operations

#### Matrix-Vector Multiplication
For matrix A (m√ón) and vector x (n√ó1):
```
Ax = [a‚ÇÅ‚ÇÅx‚ÇÅ + a‚ÇÅ‚ÇÇx‚ÇÇ + ... + a‚ÇÅ‚Çôx‚Çô]
     [a‚ÇÇ‚ÇÅx‚ÇÅ + a‚ÇÇ‚ÇÇx‚ÇÇ + ... + a‚ÇÇ‚Çôx‚Çô]
     [‚ãÆ                           ]
     [a‚Çò‚ÇÅx‚ÇÅ + a‚Çò‚ÇÇx‚ÇÇ + ... + a‚Çò‚Çôx‚Çô]
```

**Linear Regression Example:**
```
Design matrix X (n√óp), coefficient vector Œ≤ (p√ó1)
Predictions: ≈∑ = XŒ≤

X = [1  x‚ÇÅ‚ÇÅ  x‚ÇÅ‚ÇÇ]     Œ≤ = [Œ≤‚ÇÄ]     ≈∑ = [Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ‚ÇÅ + Œ≤‚ÇÇx‚ÇÅ‚ÇÇ]
    [1  x‚ÇÇ‚ÇÅ  x‚ÇÇ‚ÇÇ]         [Œ≤‚ÇÅ]         [Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÇ‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ‚ÇÇ]
    [‚ãÆ  ‚ãÆ    ‚ãÆ  ]         [Œ≤‚ÇÇ]         [‚ãÆ                  ]
    [1  x‚Çô‚ÇÅ  x‚Çô‚ÇÇ]                      [Œ≤‚ÇÄ + Œ≤‚ÇÅx‚Çô‚ÇÅ + Œ≤‚ÇÇx‚Çô‚ÇÇ]
```

#### Matrix Multiplication
For A (m√ók) and B (k√ón), product C = AB is (m√ón):
```
c·µ¢‚±º = Œ£‚Çñ a·µ¢‚Çñb‚Çñ‚±º
```

**Computational Complexity:** O(mkn)

### Special Matrices and Properties

#### Identity Matrix
```
I = [1  0  0]
    [0  1  0]  ‚üπ AI = IA = A
    [0  0  1]
```

#### Matrix Transpose
```
If A = [1  2  3]  then A·µÄ = [1  4]
       [4  5  6]            [2  5]
                            [3  6]
```

**Properties:**
- (A·µÄ)·µÄ = A
- (AB)·µÄ = B·µÄA·µÄ
- (A + B)·µÄ = A·µÄ + B·µÄ

#### Matrix Inverse
For square matrix A, inverse A‚Åª¬π satisfies AA‚Åª¬π = A‚Åª¬πA = I

**Normal Equation in Regression:**
```
Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄy
```

**Warning:** X·µÄX must be invertible (full rank condition)

### Eigenvalues and Eigenvectors

#### Definition
For square matrix A, eigenvector v and eigenvalue Œª satisfy:
```
Av = Œªv
```

**Geometric Interpretation:** A stretches v by factor Œª without changing direction.

#### Computing Eigenvalues
Solve characteristic equation:
```
det(A - ŒªI) = 0
```

**Example:**
```
A = [3  1]
    [0  2]

det([3-Œª   1  ]) = (3-Œª)(2-Œª) = Œª¬≤ - 5Œª + 6 = 0
   ([0   2-Œª])

Œª‚ÇÅ = 3, Œª‚ÇÇ = 2
```

#### Principal Component Analysis (PCA)
PCA finds eigenvectors of covariance matrix:
```
Cov(X) = (1/(n-1))X·µÄX
```
- **Principal components** = eigenvectors
- **Explained variance** = eigenvalues

### Matrix Decompositions

#### Eigenvalue Decomposition
For symmetric matrix A:
```
A = QŒõQ·µÄ
```
where Q contains eigenvectors, Œõ contains eigenvalues.

#### Singular Value Decomposition (SVD)
For any matrix A (m√ón):
```
A = UŒ£V·µÄ
```
where:
- U (m√óm): Left singular vectors (orthogonal)
- Œ£ (m√ón): Diagonal matrix of singular values
- V (n√ón): Right singular vectors (orthogonal)

**ML Applications:**
- **Dimensionality reduction**: Keep top k singular values
- **Matrix completion**: Reconstruct missing entries
- **Pseudoinverse**: A‚Å∫ = VŒ£‚Å∫U·µÄ

## Probability Theory Review

### Basic Probability

#### Sample Space and Events
- **Sample space Œ©**: All possible outcomes
- **Event A**: Subset of Œ©
- **Probability P(A)**: Measure of likelihood, 0 ‚â§ P(A) ‚â§ 1

#### Conditional Probability
```
P(A|B) = P(A ‚à© B) / P(B)
```

**Chain Rule:**
```
P(A‚ÇÅ, A‚ÇÇ, ..., A‚Çô) = P(A‚ÇÅ)P(A‚ÇÇ|A‚ÇÅ)P(A‚ÇÉ|A‚ÇÅ,A‚ÇÇ)...P(A‚Çô|A‚ÇÅ,...,A‚Çô‚Çã‚ÇÅ)
```

### Bayes' Theorem

#### Formula
```
P(H|D) = P(D|H)P(H) / P(D)
```

**Components:**
- **P(H|D)**: Posterior probability (what we want)
- **P(D|H)**: Likelihood (how well hypothesis explains data)
- **P(H)**: Prior probability (initial belief)
- **P(D)**: Evidence (normalizing constant)

#### Medical Diagnosis Example
```
Disease prevalence: P(Disease) = 0.001
Test sensitivity: P(+|Disease) = 0.99
Test specificity: P(-|No Disease) = 0.95

P(Disease|+) = P(+|Disease)P(Disease) / P(+)
             = (0.99)(0.001) / [(0.99)(0.001) + (0.05)(0.999)]
             = 0.00099 / (0.00099 + 0.04995)
             = 0.0194
```

**Surprising Result:** Only 1.94% chance of having disease despite positive test!

### Random Variables and Distributions

#### Discrete Random Variables
**Probability Mass Function (PMF):**
```
p(x) = P(X = x)
```

**Bernoulli Distribution:**
```
X ~ Bernoulli(p)
P(X = 1) = p, P(X = 0) = 1-p
E[X] = p, Var(X) = p(1-p)
```

**Binomial Distribution:**
```
X ~ Binomial(n, p)
P(X = k) = C(n,k)p·µè(1-p)‚Åø‚Åª·µè
E[X] = np, Var(X) = np(1-p)
```

#### Continuous Random Variables
**Probability Density Function (PDF):**
```
P(a ‚â§ X ‚â§ b) = ‚à´‚Çê·µá f(x)dx
```

**Normal Distribution:**
```
X ~ N(Œº, œÉ¬≤)
f(x) = (1/‚àö(2œÄœÉ¬≤))exp(-(x-Œº)¬≤/(2œÉ¬≤))
```

**Standard Normal:** Z ~ N(0,1)
**Standardization:** Z = (X - Œº)/œÉ

### Expectation and Variance

#### Expected Value
**Discrete:** E[X] = Œ£‚Çì x¬∑P(X = x)
**Continuous:** E[X] = ‚à´ x¬∑f(x)dx

**Properties:**
- E[aX + b] = aE[X] + b (linearity)
- E[X + Y] = E[X] + E[Y] (additivity)

#### Variance
```
Var(X) = E[(X - Œº)¬≤] = E[X¬≤] - (E[X])¬≤
```

**Properties:**
- Var(aX + b) = a¬≤Var(X)
- If X ‚ä• Y: Var(X + Y) = Var(X) + Var(Y)

### Central Limit Theorem

#### Statement
For independent, identically distributed X‚ÇÅ, X‚ÇÇ, ..., X‚Çô with mean Œº and variance œÉ¬≤:
```
(XÃÑ - Œº)/(œÉ/‚àön) ‚Üí·µà N(0,1) as n ‚Üí ‚àû
```

**Practical Implication:** Sample means are approximately normal for large n, regardless of population distribution.

**Example:** Rolling dice
```
Population: Uniform{1,2,3,4,5,6}
Œº = 3.5, œÉ¬≤ = 35/12

Sample mean of n=100 rolls:
XÃÑ ~ N(3.5, 35/(12√ó100)) = N(3.5, 0.0292)
```

## Statistics Fundamentals

### Maximum Likelihood Estimation

#### Concept
Find parameter values that make observed data most likely.

**Likelihood Function:**
```
L(Œ∏) = ‚àè·µ¢ f(x·µ¢|Œ∏)
```

**Log-likelihood:**
```
‚Ñì(Œ∏) = log L(Œ∏) = Œ£·µ¢ log f(x·µ¢|Œ∏)
```

#### Normal Distribution Example
For X‚ÇÅ, ..., X‚Çô ~ N(Œº, œÉ¬≤):
```
‚Ñì(Œº,œÉ¬≤) = -n/2 log(2œÄœÉ¬≤) - (1/(2œÉ¬≤))Œ£·µ¢(x·µ¢-Œº)¬≤

‚àÇ‚Ñì/‚àÇŒº = 0 ‚üπ ŒºÃÇ = (1/n)Œ£·µ¢x·µ¢ = xÃÑ
‚àÇ‚Ñì/‚àÇœÉ¬≤ = 0 ‚üπ œÉÃÇ¬≤ = (1/n)Œ£·µ¢(x·µ¢-ŒºÃÇ)¬≤
```

### Confidence Intervals

#### For Population Mean (Known œÉ)
```
xÃÑ ¬± z_{Œ±/2} √ó (œÉ/‚àön)
```

#### For Population Mean (Unknown œÉ)
```
xÃÑ ¬± t_{Œ±/2,n-1} √ó (s/‚àön)
```

**Interpretation:** We are (1-Œ±)√ó100% confident that true Œº lies in this interval.

### Hypothesis Testing

#### Framework
1. **Null Hypothesis H‚ÇÄ**: Status quo
2. **Alternative H‚ÇÅ**: What we want to prove  
3. **Test statistic**: Measures evidence against H‚ÇÄ
4. **P-value**: P(observe data as extreme | H‚ÇÄ true)
5. **Decision**: Reject H‚ÇÄ if p-value < Œ±

#### t-test Example
```
H‚ÇÄ: Œº = Œº‚ÇÄ
H‚ÇÅ: Œº ‚â† Œº‚ÇÄ

Test statistic: t = (xÃÑ - Œº‚ÇÄ)/(s/‚àön)
Under H‚ÇÄ: t ~ t(n-1)
```

## Applications in Statistical ML

### Linear Models
**Matrix formulation:**
```
y = XŒ≤ + Œµ, where Œµ ~ N(0, œÉ¬≤I)
```

**Assumptions:**
1. Linearity: E[y|X] = XŒ≤
2. Independence: Cov(Œµ·µ¢, Œµ‚±º) = 0 for i ‚â† j
3. Homoscedasticity: Var(Œµ·µ¢) = œÉ¬≤ for all i
4. Normality: Œµ ~ N(0, œÉ¬≤I)

### Regularization
**Ridge Regression (L2 penalty):**
```
Œ≤ÃÇ·µ£·µ¢ùíπùëî‚Çë = argmin_Œ≤ ||y - XŒ≤||‚ÇÇ¬≤ + Œª||Œ≤||‚ÇÇ¬≤
Œ≤ÃÇ·µ£·µ¢ùíπùëî‚Çë = (X·µÄX + ŒªI)‚Åª¬πX·µÄy
```

**Lasso Regression (L1 penalty):**
```
Œ≤ÃÇ‚Çó‚Çê‚Çõ‚Çõ‚Çí = argmin_Œ≤ ||y - XŒ≤||‚ÇÇ¬≤ + Œª||Œ≤||‚ÇÅ
```

### Principal Component Analysis
1. **Center data**: XÃÉ = X - XÃÑ
2. **Compute covariance**: C = (1/(n-1))XÃÉ·µÄXÃÉ
3. **Find eigenvectors**: C = QŒõQ·µÄ
4. **Transform data**: Y = XÃÉQ

## Computational Considerations

### Matrix Inversion
- **Computational complexity**: O(n¬≥)
- **Numerical stability**: Use QR decomposition or SVD instead
- **Condition number**: Œ∫(A) = œÉ‚Çò‚Çê‚Çì/œÉ‚Çò·µ¢‚Çô (high = ill-conditioned)

### Large-Scale Problems
- **Gradient descent**: Iterative optimization
- **Stochastic methods**: Use subsets of data
- **Matrix factorizations**: Reduce dimensionality

## Common Pitfalls

### Linear Algebra
1. **Non-invertible matrices**: Check rank before inverting
2. **Numerical precision**: Floating-point errors accumulate
3. **Memory requirements**: O(n¬≤) for n√ón matrices

### Probability
1. **Independence assumption**: Often violated in real data
2. **Distribution assumptions**: Check with diagnostic plots
3. **Sample size**: CLT requires "large" n (usually n ‚â• 30)

### Statistics
1. **Multiple testing**: Adjust for multiple comparisons
2. **Correlation ‚â† Causation**: Need experimental design
3. **Outliers**: Can dramatically affect results

## Key Takeaways

1. **Linear algebra** provides the computational framework
2. **Probability** quantifies uncertainty and randomness
3. **Statistics** enables inference from data to population
4. **Matrix operations** are fundamental to ML algorithms
5. **Assumptions matter** - always validate before proceeding
6. **Computational efficiency** becomes critical with large data

These mathematical foundations underpin all statistical machine learning methods we'll encounter in CPSC 540.